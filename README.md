<p style="border:1px; border-style:solid; border-color:black; padding: 1em;">
CS-UH 3260 Software Analytics<br/>
Replication Study<br/>
John Yun Moe & Zavier Shaikh, NYUAD
</p>

# Replication Study: Impact of CI on Pull Request Delivery Time

## 1. Project Title and Overview

- **Paper Title**: Studying the Impact of Adopting Continuous Integration on the Delivery Time of Pull Requests
- **Authors**: João Helis Bernardo, Daniel Alencar da Costa, Uirá Kulesza
- **Conference**: MSR '18: 15th International Conference on Mining Software Repositories (May 2018)
- **Replication Team**: John Yun Moe, Zavier Shaikh
- **Course**: CS-UH 3260 Software Analytics, NYUAD

### Brief Description

**Original Paper**: The paper empirically investigates how adopting Continuous Integration (CI) impacts the delivery time of pull requests in GitHub projects. Through analysis of 162,653 pull requests across 87 projects in 5 programming languages, the authors examine whether CI actually speeds up PR delivery, what factors influence delivery time, and how development activity changes after CI adoption.

**Replication Study**: This replication study aims to reproduce the key findings of the original paper by analyzing the relationship between CI adoption and pull request delivery time in GitHub projects. We examine the merge time, delivery phases, and factors that influence the time-to-delivery of merged PRs before and after CI adoption.

## 2. Repository Structure
```
README.md                 # This documentation file
datasets/                 # Collected Data from the data mining scripts and provided data from the original authors
replication_scripts/      # Python scripts for GitHub data collection
outputs/                  # Generated analysis outputs (figures, tables, models)
logs/                     # Console outputs and execution logs
notes/                    # Notes on discrepancies and observations
```

### Folder Descriptions

- **datasets/**: Contains two subfolders:
  - **Provided Data/** with baseline metadata files from the study context:
    - `pull_requests_meta_data.csv`
    - `releases_meta_data.csv`
  - **Collected Data/** with data collected by our scripts:
    - `pull_requests.csv`
    - `releases.csv`
- **replication_scripts/**: Contains Python data collection scripts and configuration:
  - `collect_pr.py`: Collects merged pull request data from GitHub repositories listed in `config.py`
  - `collect_release.py`: Collects stable GitHub release data and computes release windows
  - `config.py`: Defines target repositories and output CSV paths
  - `requirements.txt`: Dependency list for script execution
- **outputs/**: Intended location for generated analysis outputs (statistical tables, figures, models)
- **logs/**: Intended location for script run logs, API call notes, and error traces
- **notes/**: Intended location for replication observations and discrepancy tracking

## 3. Setup Instructions

### Prerequisites

- Operating System: Windows, macOS, or Linux (current setup used for this project: Windows)
- Programming Language: Python 3.10+
- Required packages/libraries:
  - `requests>=2.31.0`
  - `python-dotenv>=1.0.0`
- GitHub API access token with access to GitHub REST API endpoints used by the scripts

### Installation Steps

1. Open a terminal in the repository root.
2. (Recommended) Create and activate a virtual environment.
3. Install dependencies:
  - `pip install -r replication_scripts/requirements.txt`
4. Create/update `replication_scripts/.env` and set:
  - `GITHUB_TOKEN=<your_token_here>`
  - Optional: `GITHUB_API_BASE=https://api.github.com`
5. Confirm repository targets and output paths in `replication_scripts/config.py`.

## 4. Reproduction Steps

The current repository reproduces the data collection stage of the replication pipeline.

1. Configure environment variables in `replication_scripts/.env` (at minimum `GITHUB_TOKEN`).
2. Run pull request collection:
  - From `replication_scripts/`: `python collect_pr.py`
  - Output: `datasets/Collected Data/pull_requests.csv`
3. Run release collection:
  - From `replication_scripts/`: `python collect_release.py`
  - (Equivalent wrapper): `python collect_releases.py`
  - Output: `datasets/Collected Data/releases.csv`

## 5. Results

Current status focuses on dataset assembly for downstream statistical replication.

- **Collected Data (generated by scripts):**
  - `pull_requests.csv`:  rows (including header), merged PR records for configured repositories.
  - `releases.csv`:  rows (including header), stable release windows and metadata.
- **Provided Data (reference/baseline):**
  - `pull_requests_meta_data.csv`: 162,655 rows (including header).
  - `releases_meta_data.csv`: 7,442 rows (including header).

The next phase is statistical replication and comparison against the original MSR'18 findings (e.g., delivery time distributions before/after CI adoption and associated explanatory factors).

## 6. GenAI Usage

**Tools Used**: Claude (Anthropic) and ChatGPT (OpenAI)

**Purpose**: 
- Structuring and fine-editing this README file to improve clarity, organization, and completeness across all documented sections.
- Formatting markdown elements to maintain a consistent and professional documentation style throughout the repository.
- Editing the report draft to revise grammatical errors, improve sentence flow, and strengthen the overall professional tone of the writing.
- Using Claude to better understand how to implement and refine the replication scripts, including ideas related to GitHub API pagination, rate-limit handling, and CSV data collection structure.


## 7. References

Bernardo, J. H., da Costa, D. A., & Kulesza, U. (2018). Studying the Impact of Adopting Continuous Integration on the Delivery Time of Pull Requests. In *Proceedings of the 15th International Conference on Mining Software Repositories* (MSR '18), 131-141. https://doi.org/10.1145/3196398.3196421

---

*Last Updated*: 17th February 2026